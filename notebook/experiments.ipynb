{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS \n",
    "from langchain.embeddings import OpenAIEmbeddings # Embedding model for converting text to vectors\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41e7cf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM model\n",
    "# You can replace \"deepseek-r1-distill-llama-70b\" with your desired model\n",
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc52eb",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e104e9",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d628de",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), \"data\", \"sample.pdf\")\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "len(documents)  # Each page is 1 document, this pdf fiile have 77 pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19dc258",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62373045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\llmops\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert documents to chunks, this is experimental process and not mandatory. It only works with text documents.\n",
    "# The text_splitter is used to split the documents into smaller chunks\n",
    "# This is useful when the documents are too large to be processed in one go\n",
    "# The RecursiveCharacterTextSplitter is a text splitter that splits the text into smaller chunks\n",
    "# chunking is required to avoid token limit issues in LLMs\n",
    "# and to improve the quality of the responses by providing more context\n",
    "# chunk_size is the number of characters in each chunk\n",
    "# chunk_overlap is the number of characters that overlap between chunks\n",
    "# length_function is used to calculate the length of the text\n",
    "# here we use len function to calculate the length of the text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150, length_function=len) # chunk_size=500\n",
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs) # 765 Total number of chunks created from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "818050e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content  # Content of the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10684972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'd:\\\\llmops\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata  # Metadata of the first chunk, includes page number and source file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df892d",
   "metadata": {},
   "source": [
    "#### Embedding / Storing this data in Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a462405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff3d27c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_model.embed_documents(docs[0].page_content)  # Embedding the first chunk\n",
    "len(embedding_model.embed_documents(docs[0].page_content)[0])  # Length of the embedding vector for the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87d9b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In memory vector database for storing embeddings\n",
    "# FAISS is a library for efficient similarity search and clustering of dense vectors\n",
    "# It is used to store the embeddings of the documents and to perform similarity search\n",
    "# FAISS we can persist the vector database to disk and load it later\n",
    "# Its not available in the cloud, so we use it for local development and testing\n",
    "# The vectorstore is used to store the embeddings of the documents and to perform similarity search\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f675a9b5",
   "metadata": {},
   "source": [
    "#### 2. Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c243ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform similarity search on the vectorstore and perform Retrieval Process i.e. find the most similar chunks to the query from the vectorstore\n",
    "# This will return the chunks that are most similar to the query    \n",
    "# By default k=4, it returns the top 4 chunks that are most similar to the query\n",
    "relevant_doc=vectorstore.similarity_search(\"llama2 finetuning benchmark results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9682e26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c34f962c-69fe-45ac-9d0b-e9225a0f7099', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\llmops\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='fc23c5fb-fb85-4fc0-8e91-f9c72b55e982', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\llmops\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='db99736c-a563-4a31-881f-375887cfa184', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\llmops\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 70, 'page_label': '71'}, page_content='65B 14.27 31.59 21.90 14.89 23.51 22.27 17.16 18.91 28.40 19.32 28.71 22.00 20.03\\nLlama 2\\n7B 16.53 31.15 22.63 15.74 26.87 19.95 15.79 19.55 25.03 18.92 21.53 22.34 20.20\\n13B 21.29 37.25 22.81 17.77 32.65 24.13 21.05 20.19 35.40 27.69 26.99 28.26 23.84\\n34B 16.76 29.63 23.36 14.38 27.43 19.49 18.54 17.31 26.38 18.73 22.78 21.66 19.04\\n70B 21.29 32.90 25.91 16.92 30.60 21.35 16.93 21.47 30.42 20.12 31.05 28.43 22.35\\nFine-tuned\\nChatGPT 0.23 0.22 0.18 0 0.19 0 0.46 0 0.13 0 0.47 0 0.66'),\n",
       " Document(id='a798d604-eaaa-429a-b175-312405d59604', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\llmops\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 6, 'page_label': '7'}, page_content='models internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.\\nIn Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety\\nbenchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The\\nresults for all the individual benchmarks are available in Section A.2.2.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23e739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65B 14.27 31.59 21.90 14.89 23.51 22.27 17.16 18.91 28.40 19.32 28.71 22.00 20.03\n",
      "Llama 2\n",
      "7B 16.53 31.15 22.63 15.74 26.87 19.95 15.79 19.55 25.03 18.92 21.53 22.34 20.20\n",
      "13B 21.29 37.25 22.81 17.77 32.65 24.13 21.05 20.19 35.40 27.69 26.99 28.26 23.84\n",
      "34B 16.76 29.63 23.36 14.38 27.43 19.49 18.54 17.31 26.38 18.73 22.78 21.66 19.04\n",
      "70B 21.29 32.90 25.91 16.92 30.60 21.35 16.93 21.47 30.42 20.12 31.05 28.43 22.35\n",
      "Fine-tuned\n",
      "ChatGPT 0.23 0.22 0.18 0 0.19 0 0.46 0 0.13 0 0.47 0 0.66\n"
     ]
    }
   ],
   "source": [
    "# print(relevant_doc[0].page_content)  # Content of the second chunk that is most similar to the query\n",
    "# print(relevant_doc[1].page_content)\n",
    "# print(relevant_doc[2].page_content)  # Content of the third chunk that is most similar to the query\n",
    "# print(relevant_doc[3].page_content)  # Content of the fourth chunk that is most similar to the query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4ad5bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs = {\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588b455",
   "metadata": {},
   "source": [
    "## 3. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b4562814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template is used to format the input to the LLM\n",
    "# It is used to create a template for the input to the LLM \n",
    "# e.g., user provide a question but before sending to LLM we added context i.e. retrieved docs content from the vectorstore\n",
    "# The below text/prompt will be sent to LLM to generate an answer.\n",
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a766d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "261d0305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template is used to format the input to the LLM\n",
    "# It is used to create a template for the input to the LLM  \n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8fdd41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to format the documents into a string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1fea05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.passthrough import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e05d081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the main chain that combines the retriever, formatter, prompt, LLM and output parser\n",
    "# It takes the question as input, retrieves the relevant documents, formats them, prompts the LLM with the formatted documents and the question, and returns the answer\n",
    "# RunnablePassthrough is used to pass the question through the chain without any modification\n",
    "# LCEL is used to create a chain of runnables that can be executed in sequence  \n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c0a9692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out what \"llama\" refers to based on the given context. Let me read through the context carefully. \n",
      "\n",
      "First, I see mentions of \"Llama 1\" and \"Llama 2.\" These seem to be models of some sort, possibly AI models. There are numbers like 7B, 13B, etc., which might refer to the size of the models in terms of parameters. The context also talks about pretraining, fine-tuning, and mentions things like GPU hours and carbon footprints, which are common in AI training discussions.\n",
      "\n",
      "Looking further, there's a section about \"Llama 2: Open Foundation and Fine-Tuned Chat Models,\" which suggests that Llama 2 is an AI model, specifically a chat model. The context mentions \"training libraries,\" \"Meta’s Research Super Cluster,\" and \"production clusters,\" which are terms related to computing resources used for training large AI models.\n",
      "\n",
      "Another part discusses the limitations of Llama 2-Chat, such as knowledge cessation post-pretraining and potential for hallucinations. These are typical challenges with large language models (LLMs). The mention of \"Reinforcement Learning with Human Feedback (RLHF)\" indicates that the model was refined using human input, a common practice in training chat models to align with desired behaviors.\n",
      "\n",
      "There's also a section comparing different models like Llama 1, Llama 2, and others, with various performance metrics. This further supports the idea that Llama refers to a series of AI models, specifically LLMs.\n",
      "\n",
      "So, putting it all together, \"llama\" in this context likely refers to a series of large language models developed by Meta, with Llama 2 being the latest version that includes a chat model. These models are trained on extensive datasets and are designed for various tasks, including conversational interactions.\n",
      "</think>\n",
      "\n",
      "Llama refers to a series of large language models (LLMs) developed by Meta. Specifically, Llama 2 is the latest iteration, which includes a chat model. These models are trained on extensive datasets and are designed for various tasks, including conversational interactions. Llama 2 was developed using advanced training techniques such as Reinforcement Learning with Human Feedback (RLHF) to improve its performance and alignment with desired behaviors.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"What is llama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b28dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
